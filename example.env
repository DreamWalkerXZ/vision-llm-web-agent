# Vision LLM API Configuration
# Copy this file to .env or set these as environment variables

# Option 1: OpenAI Compatible API (e.g. Qwen/Qwen3-VL-8B-Instruct)
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.siliconflow.cn/v1
OPENAI_MODEL=qwen3-vl-8b-instruct
OPENAI_LANGUAGE_MODEL=qwen-flash

# Option 2: Local vLLM Server
# OPENAI_BASE_URL=http://localhost:8000/v1
# OPENAI_API_KEY=EMPTY
# OPENAI_MODEL=Qwen/Qwen2-VL-7B-Instruct
# OPENAI_MODEL=Qwen/Qwen2-7B-Instruct

# Option 3: Ollama
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_API_KEY=ollama
# OPENAI_MODEL=llama3.2-vision
# OPENAI_MODEL=llama3.2

# Agent Configuration (Optional)
# MAX_ROUNDS=20
# TIMEOUT_PER_ROUND=30
# ARTIFACTS_DIR=artifacts
